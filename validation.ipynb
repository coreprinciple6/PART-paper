{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# test_books = pd.read_csv('local_data/book_test.csv')\n",
    "# test_mails = pd.read_csv('local_data/mail_test.csv')\n",
    "test_blogs = pd.read_csv('local_data/blog_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pretokenized_text</th>\n",
       "      <th>decoded_text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>authoryearofbirth</th>\n",
       "      <th>authoryearofdeath</th>\n",
       "      <th>language</th>\n",
       "      <th>downloads</th>\n",
       "      <th>subjects</th>\n",
       "      <th>old_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>book_14</td>\n",
       "      <td>[0, 1187, 41732, 6, 5, 754, 14, 44959, 74, 210...</td>\n",
       "      <td>&lt;s&gt;ndoubtedly, the fact that Socialism would r...</td>\n",
       "      <td>The Soul of Man under Socialism</td>\n",
       "      <td>Wilde, Oscar</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>396</td>\n",
       "      <td>{'Socialism'}</td>\n",
       "      <td>PG1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>book_14</td>\n",
       "      <td>[167, 54, 50118, 10800, 33037, 1070, 24, 6, 98...</td>\n",
       "      <td>those who\\ncontemplated it, so, in the presen...</td>\n",
       "      <td>The Soul of Man under Socialism</td>\n",
       "      <td>Wilde, Oscar</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>396</td>\n",
       "      <td>{'Socialism'}</td>\n",
       "      <td>PG1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>book_14</td>\n",
       "      <td>[122, 19, 559, 50118, 11017, 131, 114, 6, 11, ...</td>\n",
       "      <td>now with political\\npower; if, in a word, we ...</td>\n",
       "      <td>The Soul of Man under Socialism</td>\n",
       "      <td>Wilde, Oscar</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>396</td>\n",
       "      <td>{'Socialism'}</td>\n",
       "      <td>PG1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>book_14</td>\n",
       "      <td>[34, 5812, 7, 224, 24, 4, 1437, 509, 23154, 24...</td>\n",
       "      <td>has begun to say it.  One hears it now from e...</td>\n",
       "      <td>The Soul of Man under Socialism</td>\n",
       "      <td>Wilde, Oscar</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>396</td>\n",
       "      <td>{'Socialism'}</td>\n",
       "      <td>PG1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>book_14</td>\n",
       "      <td>[50118, 241, 11312, 6514, 6, 16, 1153, 10, 588...</td>\n",
       "      <td>\\nrebellious, is probably a real personality, ...</td>\n",
       "      <td>The Soul of Man under Socialism</td>\n",
       "      <td>Wilde, Oscar</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>1900.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>396</td>\n",
       "      <td>{'Socialism'}</td>\n",
       "      <td>PG1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53560</th>\n",
       "      <td>book_960</td>\n",
       "      <td>[1195, 14, 578, 3341, 50118, 1437, 1437, 1437,...</td>\n",
       "      <td>rather that—like\\n    Rome, and Athens, and T...</td>\n",
       "      <td>Crying for the Light; Or, Fifty Years Ago. Vol...</td>\n",
       "      <td>Ritchie, J. Ewing (James Ewing)</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>2</td>\n",
       "      <td>{'England -- Social conditions -- 19th century...</td>\n",
       "      <td>PG36810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53561</th>\n",
       "      <td>book_960</td>\n",
       "      <td>[19171, 4051, 50118, 1437, 1437, 1437, 1787, 6...</td>\n",
       "      <td>usting coal\\n    supply, an overwhelming debt,...</td>\n",
       "      <td>Crying for the Light; Or, Fifty Years Ago. Vol...</td>\n",
       "      <td>Ritchie, J. Ewing (James Ewing)</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>2</td>\n",
       "      <td>{'England -- Social conditions -- 19th century...</td>\n",
       "      <td>PG36810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53562</th>\n",
       "      <td>book_960</td>\n",
       "      <td>[1437, 1437, 1437, 1437, 1437, 1437, 1437, 143...</td>\n",
       "      <td>THE END.\\n                           ...</td>\n",
       "      <td>Crying for the Light; Or, Fifty Years Ago. Vol...</td>\n",
       "      <td>Ritchie, J. Ewing (James Ewing)</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>2</td>\n",
       "      <td>{'England -- Social conditions -- 19th century...</td>\n",
       "      <td>PG36810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53563</th>\n",
       "      <td>book_960</td>\n",
       "      <td>[9, 203, 937, 773, 4, 17, 27, 578, 1215, 26339...</td>\n",
       "      <td>of much general interest.’—_Daily Chronicle_....</td>\n",
       "      <td>Crying for the Light; Or, Fifty Years Ago. Vol...</td>\n",
       "      <td>Ritchie, J. Ewing (James Ewing)</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>2</td>\n",
       "      <td>{'England -- Social conditions -- 19th century...</td>\n",
       "      <td>PG36810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53564</th>\n",
       "      <td>book_960</td>\n",
       "      <td>[50118, 560, 3794, 4, 1437, 96, 5, 173, 137, 2...</td>\n",
       "      <td>\\nto flag.  In the work before us, which is no...</td>\n",
       "      <td>Crying for the Light; Or, Fifty Years Ago. Vol...</td>\n",
       "      <td>Ritchie, J. Ewing (James Ewing)</td>\n",
       "      <td>1820.0</td>\n",
       "      <td>1898.0</td>\n",
       "      <td>['en']</td>\n",
       "      <td>2</td>\n",
       "      <td>{'England -- Social conditions -- 19th century...</td>\n",
       "      <td>PG36810</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53565 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                  pretokenized_text  \\\n",
       "0       book_14  [0, 1187, 41732, 6, 5, 754, 14, 44959, 74, 210...   \n",
       "1       book_14  [167, 54, 50118, 10800, 33037, 1070, 24, 6, 98...   \n",
       "2       book_14  [122, 19, 559, 50118, 11017, 131, 114, 6, 11, ...   \n",
       "3       book_14  [34, 5812, 7, 224, 24, 4, 1437, 509, 23154, 24...   \n",
       "4       book_14  [50118, 241, 11312, 6514, 6, 16, 1153, 10, 588...   \n",
       "...         ...                                                ...   \n",
       "53560  book_960  [1195, 14, 578, 3341, 50118, 1437, 1437, 1437,...   \n",
       "53561  book_960  [19171, 4051, 50118, 1437, 1437, 1437, 1787, 6...   \n",
       "53562  book_960  [1437, 1437, 1437, 1437, 1437, 1437, 1437, 143...   \n",
       "53563  book_960  [9, 203, 937, 773, 4, 17, 27, 578, 1215, 26339...   \n",
       "53564  book_960  [50118, 560, 3794, 4, 1437, 96, 5, 173, 137, 2...   \n",
       "\n",
       "                                            decoded_text  \\\n",
       "0      <s>ndoubtedly, the fact that Socialism would r...   \n",
       "1       those who\\ncontemplated it, so, in the presen...   \n",
       "2       now with political\\npower; if, in a word, we ...   \n",
       "3       has begun to say it.  One hears it now from e...   \n",
       "4      \\nrebellious, is probably a real personality, ...   \n",
       "...                                                  ...   \n",
       "53560   rather that—like\\n    Rome, and Athens, and T...   \n",
       "53561  usting coal\\n    supply, an overwhelming debt,...   \n",
       "53562           THE END.\\n                           ...   \n",
       "53563   of much general interest.’—_Daily Chronicle_....   \n",
       "53564  \\nto flag.  In the work before us, which is no...   \n",
       "\n",
       "                                                   title  \\\n",
       "0                        The Soul of Man under Socialism   \n",
       "1                        The Soul of Man under Socialism   \n",
       "2                        The Soul of Man under Socialism   \n",
       "3                        The Soul of Man under Socialism   \n",
       "4                        The Soul of Man under Socialism   \n",
       "...                                                  ...   \n",
       "53560  Crying for the Light; Or, Fifty Years Ago. Vol...   \n",
       "53561  Crying for the Light; Or, Fifty Years Ago. Vol...   \n",
       "53562  Crying for the Light; Or, Fifty Years Ago. Vol...   \n",
       "53563  Crying for the Light; Or, Fifty Years Ago. Vol...   \n",
       "53564  Crying for the Light; Or, Fifty Years Ago. Vol...   \n",
       "\n",
       "                                author  authoryearofbirth  authoryearofdeath  \\\n",
       "0                         Wilde, Oscar             1854.0             1900.0   \n",
       "1                         Wilde, Oscar             1854.0             1900.0   \n",
       "2                         Wilde, Oscar             1854.0             1900.0   \n",
       "3                         Wilde, Oscar             1854.0             1900.0   \n",
       "4                         Wilde, Oscar             1854.0             1900.0   \n",
       "...                                ...                ...                ...   \n",
       "53560  Ritchie, J. Ewing (James Ewing)             1820.0             1898.0   \n",
       "53561  Ritchie, J. Ewing (James Ewing)             1820.0             1898.0   \n",
       "53562  Ritchie, J. Ewing (James Ewing)             1820.0             1898.0   \n",
       "53563  Ritchie, J. Ewing (James Ewing)             1820.0             1898.0   \n",
       "53564  Ritchie, J. Ewing (James Ewing)             1820.0             1898.0   \n",
       "\n",
       "      language  downloads                                           subjects  \\\n",
       "0       ['en']        396                                      {'Socialism'}   \n",
       "1       ['en']        396                                      {'Socialism'}   \n",
       "2       ['en']        396                                      {'Socialism'}   \n",
       "3       ['en']        396                                      {'Socialism'}   \n",
       "4       ['en']        396                                      {'Socialism'}   \n",
       "...        ...        ...                                                ...   \n",
       "53560   ['en']          2  {'England -- Social conditions -- 19th century...   \n",
       "53561   ['en']          2  {'England -- Social conditions -- 19th century...   \n",
       "53562   ['en']          2  {'England -- Social conditions -- 19th century...   \n",
       "53563   ['en']          2  {'England -- Social conditions -- 19th century...   \n",
       "53564   ['en']          2  {'England -- Social conditions -- 19th century...   \n",
       "\n",
       "        old_id  \n",
       "0       PG1017  \n",
       "1       PG1017  \n",
       "2       PG1017  \n",
       "3       PG1017  \n",
       "4       PG1017  \n",
       "...        ...  \n",
       "53560  PG36810  \n",
       "53561  PG36810  \n",
       "53562  PG36810  \n",
       "53563  PG36810  \n",
       "53564  PG36810  \n",
       "\n",
       "[53565 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pretokenized_text</th>\n",
       "      <th>decoded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mail_105</td>\n",
       "      <td>[0, 1620, 47, 32, 2542, 6, 110, 265, 1933, 34,...</td>\n",
       "      <td>&lt;s&gt;As you are aware, your business unit has be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mail_105</td>\n",
       "      <td>[16410, 4383, 50118, 28062, 12552, 16, 5, 1850...</td>\n",
       "      <td>Methodology\\nAttached is the proposed ERCOT I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mail_105</td>\n",
       "      <td>[8, 5214, 844, 50118, 627, 595, 183, 1914, 239...</td>\n",
       "      <td>and=20\\nthe current day forecast high tempera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mail_105</td>\n",
       "      <td>[2388, 87, 5, 4971, 6, 172, 5, 230, 3632, 3500...</td>\n",
       "      <td>greater than the limits, then the CSC proble=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mail_105</td>\n",
       "      <td>[381, 5199, 3293, 4, 5457, 844, 50118, 14773, ...</td>\n",
       "      <td>ERCOT. =20\\nMarket rules dictate that by 0600...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123787</th>\n",
       "      <td>mail_56</td>\n",
       "      <td>[515, 322, 1437, 1437, 3401, 680, 162, 1437, 5...</td>\n",
       "      <td>event).   Please include me \\nwhen \\nyou give...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123788</th>\n",
       "      <td>mail_56</td>\n",
       "      <td>[16135, 7, 5, 1248, 644, 1105, 6, 3788, 4, 143...</td>\n",
       "      <td>forward to the date January 31, 2000.  It is b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123789</th>\n",
       "      <td>mail_56</td>\n",
       "      <td>[1189, 5, 276, 6, 53, 5, 433, 16, 533, 7, 422,...</td>\n",
       "      <td>remains the same, but the media is likely to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123790</th>\n",
       "      <td>mail_56</td>\n",
       "      <td>[530, 13673, 12859, 7, 10, 2257, 31482, 11, 10...</td>\n",
       "      <td>K bug refers to a software glitch in some olde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123791</th>\n",
       "      <td>mail_56</td>\n",
       "      <td>[50118, 133, 15771, 561, 1787, 823, 130, 153, ...</td>\n",
       "      <td>\\nThe pipelines together supply nearly three m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>123792 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                  pretokenized_text  \\\n",
       "0       mail_105  [0, 1620, 47, 32, 2542, 6, 110, 265, 1933, 34,...   \n",
       "1       mail_105  [16410, 4383, 50118, 28062, 12552, 16, 5, 1850...   \n",
       "2       mail_105  [8, 5214, 844, 50118, 627, 595, 183, 1914, 239...   \n",
       "3       mail_105  [2388, 87, 5, 4971, 6, 172, 5, 230, 3632, 3500...   \n",
       "4       mail_105  [381, 5199, 3293, 4, 5457, 844, 50118, 14773, ...   \n",
       "...          ...                                                ...   \n",
       "123787   mail_56  [515, 322, 1437, 1437, 3401, 680, 162, 1437, 5...   \n",
       "123788   mail_56  [16135, 7, 5, 1248, 644, 1105, 6, 3788, 4, 143...   \n",
       "123789   mail_56  [1189, 5, 276, 6, 53, 5, 433, 16, 533, 7, 422,...   \n",
       "123790   mail_56  [530, 13673, 12859, 7, 10, 2257, 31482, 11, 10...   \n",
       "123791   mail_56  [50118, 133, 15771, 561, 1787, 823, 130, 153, ...   \n",
       "\n",
       "                                             decoded_text  \n",
       "0       <s>As you are aware, your business unit has be...  \n",
       "1        Methodology\\nAttached is the proposed ERCOT I...  \n",
       "2        and=20\\nthe current day forecast high tempera...  \n",
       "3        greater than the limits, then the CSC proble=...  \n",
       "4        ERCOT. =20\\nMarket rules dictate that by 0600...  \n",
       "...                                                   ...  \n",
       "123787   event).   Please include me \\nwhen \\nyou give...  \n",
       "123788  forward to the date January 31, 2000.  It is b...  \n",
       "123789   remains the same, but the media is likely to ...  \n",
       "123790  K bug refers to a software glitch in some olde...  \n",
       "123791  \\nThe pipelines together supply nearly three m...  \n",
       "\n",
       "[123792 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pretokenized_text</th>\n",
       "      <th>decoded_text</th>\n",
       "      <th>age</th>\n",
       "      <th>topic</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blog_10002</td>\n",
       "      <td>[0, 250, 14125, 2081, 23, 5, 502, 158, 6, 4482...</td>\n",
       "      <td>&lt;s&gt;A pivotal exchange at the June 10, 2004 pre...</td>\n",
       "      <td>43</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blog_10002</td>\n",
       "      <td>[18984, 396, 878, 9724, 5156, 9, 5, 488, 4, 14...</td>\n",
       "      <td>detainees without running afoul of the law.  ...</td>\n",
       "      <td>43</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blog_10002</td>\n",
       "      <td>[86, 4, 5359, 38, 64, 28, 55, 699, 4, 20, 9223...</td>\n",
       "      <td>time. Maybe I can be more clear. The instruct...</td>\n",
       "      <td>43</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blog_10002</td>\n",
       "      <td>[58, 5, 9223, 31, 162, 7, 5, 168, 4, 1437, 407...</td>\n",
       "      <td>were the instructions from me to the governme...</td>\n",
       "      <td>43</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blog_10002</td>\n",
       "      <td>[6, 1256, 203, 20154, 128, 41010, 108, 11, 70,...</td>\n",
       "      <td>, pretty much spells 'YES' in all caps.  So wh...</td>\n",
       "      <td>43</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281508</th>\n",
       "      <td>blog_9992</td>\n",
       "      <td>[26, 205, 15687, 4, 178, 54, 40, 1798, 5, 2597...</td>\n",
       "      <td>said good bye. And who will hear the echoes o...</td>\n",
       "      <td>14</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281509</th>\n",
       "      <td>blog_9992</td>\n",
       "      <td>[8, 4318, 4, 16304, 62, 5, 13171, 9, 20299, 4,...</td>\n",
       "      <td>and mine. Adding up the layers of harmony. An...</td>\n",
       "      <td>14</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281510</th>\n",
       "      <td>blog_9992</td>\n",
       "      <td>[127, 6180, 8, 7416, 2115, 167, 11954, 4, 1508...</td>\n",
       "      <td>my memories and dreams upon those wings. Leav...</td>\n",
       "      <td>14</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281511</th>\n",
       "      <td>blog_9992</td>\n",
       "      <td>[24, 1411, 6, 15, 8, 15, 4, 4448, 27799, 9, 30...</td>\n",
       "      <td>it goes, on and on. Melodies of life,To the s...</td>\n",
       "      <td>14</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281512</th>\n",
       "      <td>blog_9992</td>\n",
       "      <td>[111, 6000, 8, 15, 4, 2]</td>\n",
       "      <td>- forever and on.&lt;/s&gt;</td>\n",
       "      <td>14</td>\n",
       "      <td>indUnk</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281513 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id                                  pretokenized_text  \\\n",
       "0       blog_10002  [0, 250, 14125, 2081, 23, 5, 502, 158, 6, 4482...   \n",
       "1       blog_10002  [18984, 396, 878, 9724, 5156, 9, 5, 488, 4, 14...   \n",
       "2       blog_10002  [86, 4, 5359, 38, 64, 28, 55, 699, 4, 20, 9223...   \n",
       "3       blog_10002  [58, 5, 9223, 31, 162, 7, 5, 168, 4, 1437, 407...   \n",
       "4       blog_10002  [6, 1256, 203, 20154, 128, 41010, 108, 11, 70,...   \n",
       "...            ...                                                ...   \n",
       "281508   blog_9992  [26, 205, 15687, 4, 178, 54, 40, 1798, 5, 2597...   \n",
       "281509   blog_9992  [8, 4318, 4, 16304, 62, 5, 13171, 9, 20299, 4,...   \n",
       "281510   blog_9992  [127, 6180, 8, 7416, 2115, 167, 11954, 4, 1508...   \n",
       "281511   blog_9992  [24, 1411, 6, 15, 8, 15, 4, 4448, 27799, 9, 30...   \n",
       "281512   blog_9992                           [111, 6000, 8, 15, 4, 2]   \n",
       "\n",
       "                                             decoded_text  age   topic gender  \n",
       "0       <s>A pivotal exchange at the June 10, 2004 pre...   43  indUnk   male  \n",
       "1        detainees without running afoul of the law.  ...   43  indUnk   male  \n",
       "2        time. Maybe I can be more clear. The instruct...   43  indUnk   male  \n",
       "3        were the instructions from me to the governme...   43  indUnk   male  \n",
       "4       , pretty much spells 'YES' in all caps.  So wh...   43  indUnk   male  \n",
       "...                                                   ...  ...     ...    ...  \n",
       "281508   said good bye. And who will hear the echoes o...   14  indUnk   male  \n",
       "281509   and mine. Adding up the layers of harmony. An...   14  indUnk   male  \n",
       "281510   my memories and dreams upon those wings. Leav...   14  indUnk   male  \n",
       "281511   it goes, on and on. Melodies of life,To the s...   14  indUnk   male  \n",
       "281512                              - forever and on.</s>   14  indUnk   male  \n",
       "\n",
       "[281513 rows x 6 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "from sklearn.metrics import top_k_accuracy_score, accuracy_score\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained('roberta-large')\n",
    "\n",
    "def embed(model, texts):\n",
    "    tokenized_texts = TOKENIZER(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    embedding = model(tokenized_texts.input_ids.to(model.device),\n",
    "                      tokenized_texts.attention_mask.to(model.device),\n",
    "                      )\n",
    "    return embedding\n",
    "\n",
    "def embed_transformer(model, texts):\n",
    "    tokenized_texts = TOKENIZER(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    embedding = model(tokenized_texts.input_ids.to(model.device),\n",
    "                      attention_mask = tokenized_texts.attention_mask.to(model.device),\n",
    "                      ).pooler_output\n",
    "    return embedding\n",
    "\n",
    "def evaluate(model, data, top_k=5, N=100, repetitions=1, embed_f=embed):\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        accs, topks = [], []\n",
    "        for _ in tqdm(range(repetitions)):           \n",
    "            authors = data.id.unique().tolist()\n",
    "            shuffle(authors)\n",
    "            random_authors = authors[:N]\n",
    "            anchors, replicas = [], []\n",
    "            for author in random_authors:\n",
    "                anchor, replica = data.loc[author == data.id].decoded_text.sample(2).tolist()\n",
    "                anchors.append(anchor)\n",
    "                replicas.append(replica)\n",
    "            \n",
    "            embedding_anchors = F.normalize(embed_f(model, anchors))\n",
    "            embedding_replicas = F.normalize(embed_f(model, replicas))\n",
    "\n",
    "            preds = embedding_anchors @ embedding_replicas.T\n",
    "            labels = torch.arange(0, len(preds)).numpy()\n",
    "\n",
    "            preds_a = F.softmax(preds, dim=-1)\n",
    "            preds_b = F.softmax(preds.T, dim=-1)\n",
    "\n",
    "            a_acc = accuracy_score(labels, preds_a.argmax(-1).cpu().numpy())\n",
    "            b_acc = accuracy_score(labels, preds_b.argmax(-1).cpu().numpy())\n",
    "            a_topk = top_k_accuracy_score(y_true=labels, y_score=preds_a.cpu().numpy(), k=top_k)\n",
    "            b_topk = top_k_accuracy_score(y_true=labels, y_score=preds_b.cpu().numpy(), k=top_k)\n",
    "\n",
    "            accs.append((a_acc+b_acc)/2)\n",
    "            topks.append((a_topk+b_topk)/2)\n",
    "\n",
    "            del embedding_anchors\n",
    "            del embedding_replicas\n",
    "\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        return np.mean(accs), np.mean(topks), np.std(accs), np.std(topks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_zoo = {#'all': 'model/final_2022-06-21_12-22-26_lstm_books+mails+blogs.ckpt',\n",
    "            # 'books': 'model/final_2022-06-28_07-55-16_lstm_books.ckpt',\n",
    "            # 'mails': 'model/final_2022-06-27_08-14-08_lstm_mails.ckpt',\n",
    "            'blogs': 'model/final_2022-06-15_08-10-17_lstm_blogs.ckpt',\n",
    "            }\n",
    "data_zoo = {#'all': test_books.append(test_mails).append(test_blogs),\n",
    "            # 'books': test_books,\n",
    "            # 'mails': test_mails,\n",
    "            'blogs': test_blogs,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] Evaluating model - RoBERTa\n",
      "[log] Evaluating on data - blogs\n",
      "[log] Running 100 repetitions for N=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39fece805f7341c4a755ed97cfb0827e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model_data:blogs_n:10: 22.10% 62.90%\n",
      "[log] Running 100 repetitions for N=20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8d753ef7c074edebc2dac40452e3243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model_data:blogs_n:20: 16.75% 40.88%\n",
      "[log] Running 100 repetitions for N=50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3379c47b1e324b278b38dd71b3d96285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     n \u001b[39m=\u001b[39m max_len\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m max_len:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     acc, topk, acc_sd, topk_sd \u001b[39m=\u001b[39m evaluate(model, data, top_k\u001b[39m=\u001b[39;49mTOP_K, N\u001b[39m=\u001b[39;49mn, repetitions\u001b[39m=\u001b[39;49mREPEATS, embed_f\u001b[39m=\u001b[39;49membed_transformer)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39m# data_dict[f'Model_{k_model}'][f'data_{k_data}'][f'N={n} Accuracy'] = f'{100*acc:.2f}% ± {100*acc_sd:.2f}'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     \u001b[39m# data_dict[f'Model_{k_model}'][f'data_{k_data}'][f'N={n} Top-5 Accuracy'] = f'{100*topk:.2f}% ± {100*topk_sd:.2f}'\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m# print(f'[log] model:{k_model}_data:{k_data}_n:{n}: {100*acc:.2f}% {100*topk:.2f}%')\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     data_dict[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata_\u001b[39m\u001b[39m{\u001b[39;00mk_data\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m][\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mN=\u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m}\u001b[39;00m\u001b[39m Accuracy\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100\u001b[39m\u001b[39m*\u001b[39macc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m% ± \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m100\u001b[39m\u001b[39m*\u001b[39macc_sd\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32m/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m preds_a \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(preds, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m preds_b \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(preds\u001b[39m.\u001b[39mT, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m a_acc \u001b[39m=\u001b[39m accuracy_score(labels, preds_a\u001b[39m.\u001b[39;49margmax(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m b_acc \u001b[39m=\u001b[39m accuracy_score(labels, preds_b\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ishratjahanananya/Documents/NLP_Lab/authorship-embeddings-main/validation.ipynb#W6sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m a_topk \u001b[39m=\u001b[39m top_k_accuracy_score(y_true\u001b[39m=\u001b[39mlabels, y_score\u001b[39m=\u001b[39mpreds_a\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy(), k\u001b[39m=\u001b[39mtop_k)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#from model_experimental import ContrastiveLSTMTransformer\n",
    "from model import ContrastiveLSTMHead\n",
    "from random import shuffle\n",
    "\n",
    "REPEATS = 100\n",
    "TOP_K = 5\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "n_list = [10, 20, 50, 100, 250]\n",
    "#keys = ['all', 'blogs', 'books', 'mails']\n",
    "keys = ['blogs']\n",
    "\n",
    "data_dict = {}\n",
    "# # Pass on trained models\n",
    "# for k_model in keys:\n",
    "#     model_ckpt = model_zoo[k_model]\n",
    "#     model = ContrastiveLSTMHead.load_from_checkpoint(checkpoint_path=model_ckpt).cuda(DEVICE)\n",
    "\n",
    "#     data_dict[f'Model_{k_model}'] = {}\n",
    "#     print(f'[log] Evaluating model - {k_model}')\n",
    "\n",
    "#     for k_data in keys:\n",
    "#         data = data_zoo[k_data]\n",
    "        \n",
    "#         data_dict[f'Model_{k_model}'][f'data_{k_data}'] = {}\n",
    "#         print(f'[log] Evaluating on data - {k_data}')\n",
    "\n",
    "#         for n in n_list:\n",
    "#             print(f'[log] Running {REPEATS} repetitions for N={n}')\n",
    "#             max_len = len(data.id.unique())\n",
    "            \n",
    "#             if n == 'max':\n",
    "#                 n = max_len\n",
    "\n",
    "#             if n <= max_len:\n",
    "#                 acc, topk, acc_sd, topk_sd = evaluate(model, data, top_k=TOP_K, N=n, repetitions=REPEATS)\n",
    "#                 data_dict[f'Model_{k_model}'][f'data_{k_data}'][f'N={n} Accuracy'] = f'{100*acc:.2f}% ± {100*acc_sd:.2f}'\n",
    "#                 data_dict[f'Model_{k_model}'][f'data_{k_data}'][f'N={n} Top-5 Accuracy'] = f'{100*topk:.2f}% ± {100*topk_sd:.2f}'\n",
    "\n",
    "#                 print(f'[log] model:{k_model}_data:{k_data}_n:{n}: {100*acc:.2f}% {100*topk:.2f}%')\n",
    "    \n",
    "#     del model\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained('roberta-large').to(DEVICE)\n",
    "\n",
    "data_dict[f'Model_RoBERTa'] = {}\n",
    "print(f'[log] Evaluating model - RoBERTa')\n",
    "#Base transformer\n",
    "for k_data in keys:\n",
    "    data = data_zoo[k_data]\n",
    "        \n",
    "    #data_dict[f'Model_{k_model}'][f'data_{k_data}'] = {}\n",
    "    data_dict[f'data_{k_data}'] = {}\n",
    "    print(f'[log] Evaluating on data - {k_data}')\n",
    "\n",
    "    for n in n_list:\n",
    "        print(f'[log] Running {REPEATS} repetitions for N={n}')\n",
    "        max_len = len(data.id.unique())\n",
    "        \n",
    "        if n == 'max':\n",
    "            n = max_len\n",
    "\n",
    "        if n <= max_len:\n",
    "            acc, topk, acc_sd, topk_sd = evaluate(model, data, top_k=TOP_K, N=n, repetitions=REPEATS, embed_f=embed_transformer)\n",
    "            # data_dict[f'Model_{k_model}'][f'data_{k_data}'][f'N={n} Accuracy'] = f'{100*acc:.2f}% ± {100*acc_sd:.2f}'\n",
    "            # data_dict[f'Model_{k_model}'][f'data_{k_data}'][f'N={n} Top-5 Accuracy'] = f'{100*topk:.2f}% ± {100*topk_sd:.2f}'\n",
    "\n",
    "            # print(f'[log] model:{k_model}_data:{k_data}_n:{n}: {100*acc:.2f}% {100*topk:.2f}%')\n",
    "\n",
    "            data_dict[f'data_{k_data}'][f'N={n} Accuracy'] = f'{100*acc:.2f}% ± {100*acc_sd:.2f}'\n",
    "            data_dict[f'data_{k_data}'][f'N={n} Top-5 Accuracy'] = f'{100*topk:.2f}% ± {100*topk_sd:.2f}'\n",
    "\n",
    "            print(f'[log] model_data:{k_data}_n:{n}: {100*acc:.2f}% {100*topk:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_all</th>\n",
       "      <th>Model_blogs</th>\n",
       "      <th>Model_books</th>\n",
       "      <th>Model_mails</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data_all</th>\n",
       "      <td>{'N=10 Accuracy': '91.25% ± 7.69', 'N=10 Top-5...</td>\n",
       "      <td>{'N=10 Accuracy': '90.35% ± 8.67', 'N=10 Top-5...</td>\n",
       "      <td>{'N=10 Accuracy': '58.25% ± 13.16', 'N=10 Top-...</td>\n",
       "      <td>{'N=10 Accuracy': '25.40% ± 11.48', 'N=10 Top-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_blogs</th>\n",
       "      <td>{'N=10 Accuracy': '90.60% ± 8.90', 'N=10 Top-5...</td>\n",
       "      <td>{'N=10 Accuracy': '91.55% ± 8.21', 'N=10 Top-5...</td>\n",
       "      <td>{'N=10 Accuracy': '52.60% ± 13.59', 'N=10 Top-...</td>\n",
       "      <td>{'N=10 Accuracy': '21.30% ± 10.38', 'N=10 Top-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_books</th>\n",
       "      <td>{'N=10 Accuracy': '87.75% ± 10.08', 'N=10 Top-...</td>\n",
       "      <td>{'N=10 Accuracy': '64.15% ± 12.71', 'N=10 Top-...</td>\n",
       "      <td>{'N=10 Accuracy': '83.50% ± 11.15', 'N=10 Top-...</td>\n",
       "      <td>{'N=10 Accuracy': '26.90% ± 12.22', 'N=10 Top-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_mails</th>\n",
       "      <td>{'N=10 Accuracy': '28.65% ± 9.79', 'N=10 Top-5...</td>\n",
       "      <td>{'N=10 Accuracy': '26.25% ± 11.76', 'N=10 Top-...</td>\n",
       "      <td>{'N=10 Accuracy': '21.80% ± 10.62', 'N=10 Top-...</td>\n",
       "      <td>{'N=10 Accuracy': '24.90% ± 12.43', 'N=10 Top-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Model_all  \\\n",
       "data_all    {'N=10 Accuracy': '91.25% ± 7.69', 'N=10 Top-5...   \n",
       "data_blogs  {'N=10 Accuracy': '90.60% ± 8.90', 'N=10 Top-5...   \n",
       "data_books  {'N=10 Accuracy': '87.75% ± 10.08', 'N=10 Top-...   \n",
       "data_mails  {'N=10 Accuracy': '28.65% ± 9.79', 'N=10 Top-5...   \n",
       "\n",
       "                                                  Model_blogs  \\\n",
       "data_all    {'N=10 Accuracy': '90.35% ± 8.67', 'N=10 Top-5...   \n",
       "data_blogs  {'N=10 Accuracy': '91.55% ± 8.21', 'N=10 Top-5...   \n",
       "data_books  {'N=10 Accuracy': '64.15% ± 12.71', 'N=10 Top-...   \n",
       "data_mails  {'N=10 Accuracy': '26.25% ± 11.76', 'N=10 Top-...   \n",
       "\n",
       "                                                  Model_books  \\\n",
       "data_all    {'N=10 Accuracy': '58.25% ± 13.16', 'N=10 Top-...   \n",
       "data_blogs  {'N=10 Accuracy': '52.60% ± 13.59', 'N=10 Top-...   \n",
       "data_books  {'N=10 Accuracy': '83.50% ± 11.15', 'N=10 Top-...   \n",
       "data_mails  {'N=10 Accuracy': '21.80% ± 10.62', 'N=10 Top-...   \n",
       "\n",
       "                                                  Model_mails  \n",
       "data_all    {'N=10 Accuracy': '25.40% ± 11.48', 'N=10 Top-...  \n",
       "data_blogs  {'N=10 Accuracy': '21.30% ± 10.38', 'N=10 Top-...  \n",
       "data_books  {'N=10 Accuracy': '26.90% ± 12.22', 'N=10 Top-...  \n",
       "data_mails  {'N=10 Accuracy': '24.90% ± 12.43', 'N=10 Top-...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results/tests_summary.json', 'w') as f:\n",
    "    json.dump(data_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('results/tests_summary.json', 'r') as f:\n",
    "    data_dict = json.load(f)\n",
    "\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_all</th>\n",
       "      <th>data_blogs</th>\n",
       "      <th>data_books</th>\n",
       "      <th>data_mails</th>\n",
       "      <th>Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>N=10 Accuracy</th>\n",
       "      <td>91.25% ± 7.69</td>\n",
       "      <td>90.60% ± 8.90</td>\n",
       "      <td>87.75% ± 10.08</td>\n",
       "      <td>28.65% ± 9.79</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Top-5 Accuracy</th>\n",
       "      <td>98.35% ± 3.47</td>\n",
       "      <td>98.25% ± 4.02</td>\n",
       "      <td>98.85% ± 3.31</td>\n",
       "      <td>68.85% ± 10.02</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Accuracy</th>\n",
       "      <td>86.60% ± 7.16</td>\n",
       "      <td>88.08% ± 6.68</td>\n",
       "      <td>81.45% ± 7.21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Top-5 Accuracy</th>\n",
       "      <td>95.40% ± 4.28</td>\n",
       "      <td>96.70% ± 3.59</td>\n",
       "      <td>96.28% ± 3.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Accuracy</th>\n",
       "      <td>83.72% ± 4.60</td>\n",
       "      <td>82.51% ± 4.78</td>\n",
       "      <td>72.14% ± 5.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Top-5 Accuracy</th>\n",
       "      <td>93.73% ± 3.54</td>\n",
       "      <td>93.13% ± 3.58</td>\n",
       "      <td>91.94% ± 3.26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Accuracy</th>\n",
       "      <td>78.39% ± 3.99</td>\n",
       "      <td>77.95% ± 3.95</td>\n",
       "      <td>65.07% ± 3.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Top-5 Accuracy</th>\n",
       "      <td>90.84% ± 2.55</td>\n",
       "      <td>90.14% ± 2.90</td>\n",
       "      <td>86.49% ± 2.81</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Accuracy</th>\n",
       "      <td>72.39% ± 2.39</td>\n",
       "      <td>71.56% ± 2.45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Top-5 Accuracy</th>\n",
       "      <td>86.73% ± 2.07</td>\n",
       "      <td>85.77% ± 1.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Accuracy</th>\n",
       "      <td>90.35% ± 8.67</td>\n",
       "      <td>91.55% ± 8.21</td>\n",
       "      <td>64.15% ± 12.71</td>\n",
       "      <td>26.25% ± 11.76</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Top-5 Accuracy</th>\n",
       "      <td>97.55% ± 4.50</td>\n",
       "      <td>97.75% ± 4.49</td>\n",
       "      <td>92.75% ± 6.72</td>\n",
       "      <td>67.65% ± 13.52</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Accuracy</th>\n",
       "      <td>86.67% ± 7.81</td>\n",
       "      <td>87.68% ± 7.95</td>\n",
       "      <td>56.17% ± 9.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Top-5 Accuracy</th>\n",
       "      <td>96.10% ± 3.99</td>\n",
       "      <td>96.05% ± 3.97</td>\n",
       "      <td>85.32% ± 6.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Accuracy</th>\n",
       "      <td>82.22% ± 5.16</td>\n",
       "      <td>82.07% ± 4.16</td>\n",
       "      <td>43.74% ± 5.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Top-5 Accuracy</th>\n",
       "      <td>93.16% ± 3.60</td>\n",
       "      <td>92.99% ± 3.19</td>\n",
       "      <td>70.78% ± 5.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Accuracy</th>\n",
       "      <td>77.37% ± 3.94</td>\n",
       "      <td>77.73% ± 3.86</td>\n",
       "      <td>35.94% ± 3.69</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Top-5 Accuracy</th>\n",
       "      <td>90.39% ± 2.99</td>\n",
       "      <td>90.00% ± 2.89</td>\n",
       "      <td>60.44% ± 4.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Accuracy</th>\n",
       "      <td>70.51% ± 2.74</td>\n",
       "      <td>71.11% ± 2.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Top-5 Accuracy</th>\n",
       "      <td>85.65% ± 2.19</td>\n",
       "      <td>85.50% ± 2.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_blogs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Accuracy</th>\n",
       "      <td>58.25% ± 13.16</td>\n",
       "      <td>52.60% ± 13.59</td>\n",
       "      <td>83.50% ± 11.15</td>\n",
       "      <td>21.80% ± 10.62</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Top-5 Accuracy</th>\n",
       "      <td>88.85% ± 9.38</td>\n",
       "      <td>87.35% ± 8.84</td>\n",
       "      <td>96.75% ± 4.82</td>\n",
       "      <td>63.40% ± 11.51</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Accuracy</th>\n",
       "      <td>45.98% ± 11.06</td>\n",
       "      <td>41.62% ± 9.77</td>\n",
       "      <td>77.92% ± 8.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Top-5 Accuracy</th>\n",
       "      <td>76.40% ± 8.26</td>\n",
       "      <td>73.10% ± 9.04</td>\n",
       "      <td>93.40% ± 4.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Accuracy</th>\n",
       "      <td>34.93% ± 6.33</td>\n",
       "      <td>30.62% ± 5.84</td>\n",
       "      <td>70.10% ± 6.62</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Top-5 Accuracy</th>\n",
       "      <td>62.19% ± 5.96</td>\n",
       "      <td>57.20% ± 6.55</td>\n",
       "      <td>89.28% ± 3.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Accuracy</th>\n",
       "      <td>28.06% ± 4.26</td>\n",
       "      <td>24.88% ± 3.35</td>\n",
       "      <td>61.38% ± 4.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Top-5 Accuracy</th>\n",
       "      <td>50.18% ± 4.94</td>\n",
       "      <td>47.44% ± 4.13</td>\n",
       "      <td>83.51% ± 2.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Accuracy</th>\n",
       "      <td>20.90% ± 2.09</td>\n",
       "      <td>17.05% ± 1.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Top-5 Accuracy</th>\n",
       "      <td>38.08% ± 2.73</td>\n",
       "      <td>33.81% ± 2.44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Accuracy</th>\n",
       "      <td>25.40% ± 11.48</td>\n",
       "      <td>21.30% ± 10.38</td>\n",
       "      <td>26.90% ± 12.22</td>\n",
       "      <td>24.90% ± 12.43</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Top-5 Accuracy</th>\n",
       "      <td>70.40% ± 14.10</td>\n",
       "      <td>68.10% ± 14.40</td>\n",
       "      <td>70.55% ± 10.84</td>\n",
       "      <td>68.80% ± 12.65</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Accuracy</th>\n",
       "      <td>18.40% ± 8.76</td>\n",
       "      <td>13.77% ± 6.26</td>\n",
       "      <td>16.93% ± 7.38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Top-5 Accuracy</th>\n",
       "      <td>47.82% ± 11.84</td>\n",
       "      <td>42.43% ± 9.20</td>\n",
       "      <td>48.10% ± 9.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Accuracy</th>\n",
       "      <td>10.47% ± 4.12</td>\n",
       "      <td>8.56% ± 3.42</td>\n",
       "      <td>8.94% ± 3.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Top-5 Accuracy</th>\n",
       "      <td>28.72% ± 6.13</td>\n",
       "      <td>24.13% ± 5.72</td>\n",
       "      <td>27.94% ± 4.98</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Accuracy</th>\n",
       "      <td>6.84% ± 1.87</td>\n",
       "      <td>5.47% ± 2.13</td>\n",
       "      <td>6.29% ± 2.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Top-5 Accuracy</th>\n",
       "      <td>19.39% ± 3.83</td>\n",
       "      <td>15.86% ± 3.45</td>\n",
       "      <td>18.55% ± 3.64</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Accuracy</th>\n",
       "      <td>4.27% ± 1.06</td>\n",
       "      <td>3.05% ± 0.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Top-5 Accuracy</th>\n",
       "      <td>11.87% ± 1.69</td>\n",
       "      <td>8.95% ± 1.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_mails</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Accuracy</th>\n",
       "      <td>44.65% ± 15.02</td>\n",
       "      <td>39.20% ± 13.61</td>\n",
       "      <td>44.05% ± 13.98</td>\n",
       "      <td>23.85% ± 10.46</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=10 Top-5 Accuracy</th>\n",
       "      <td>76.55% ± 11.24</td>\n",
       "      <td>72.50% ± 9.39</td>\n",
       "      <td>77.90% ± 10.82</td>\n",
       "      <td>63.00% ± 11.45</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Accuracy</th>\n",
       "      <td>36.95% ± 10.37</td>\n",
       "      <td>34.42% ± 8.93</td>\n",
       "      <td>37.15% ± 9.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=20 Top-5 Accuracy</th>\n",
       "      <td>61.33% ± 9.41</td>\n",
       "      <td>58.30% ± 8.56</td>\n",
       "      <td>63.62% ± 8.03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Accuracy</th>\n",
       "      <td>27.65% ± 5.61</td>\n",
       "      <td>24.53% ± 4.80</td>\n",
       "      <td>28.91% ± 6.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=50 Top-5 Accuracy</th>\n",
       "      <td>45.89% ± 6.49</td>\n",
       "      <td>41.34% ± 6.19</td>\n",
       "      <td>50.39% ± 6.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Accuracy</th>\n",
       "      <td>23.30% ± 4.01</td>\n",
       "      <td>21.30% ± 3.82</td>\n",
       "      <td>24.23% ± 3.31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=100 Top-5 Accuracy</th>\n",
       "      <td>38.09% ± 4.46</td>\n",
       "      <td>34.96% ± 4.17</td>\n",
       "      <td>42.11% ± 3.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Accuracy</th>\n",
       "      <td>18.77% ± 1.99</td>\n",
       "      <td>16.76% ± 1.86</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N=250 Top-5 Accuracy</th>\n",
       "      <td>30.03% ± 2.29</td>\n",
       "      <td>26.94% ± 2.49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Model_RoBERTa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            data_all      data_blogs      data_books  \\\n",
       "N=10 Accuracy          91.25% ± 7.69   90.60% ± 8.90  87.75% ± 10.08   \n",
       "N=10 Top-5 Accuracy    98.35% ± 3.47   98.25% ± 4.02   98.85% ± 3.31   \n",
       "N=20 Accuracy          86.60% ± 7.16   88.08% ± 6.68   81.45% ± 7.21   \n",
       "N=20 Top-5 Accuracy    95.40% ± 4.28   96.70% ± 3.59   96.28% ± 3.86   \n",
       "N=50 Accuracy          83.72% ± 4.60   82.51% ± 4.78   72.14% ± 5.34   \n",
       "N=50 Top-5 Accuracy    93.73% ± 3.54   93.13% ± 3.58   91.94% ± 3.26   \n",
       "N=100 Accuracy         78.39% ± 3.99   77.95% ± 3.95   65.07% ± 3.89   \n",
       "N=100 Top-5 Accuracy   90.84% ± 2.55   90.14% ± 2.90   86.49% ± 2.81   \n",
       "N=250 Accuracy         72.39% ± 2.39   71.56% ± 2.45             NaN   \n",
       "N=250 Top-5 Accuracy   86.73% ± 2.07   85.77% ± 1.92             NaN   \n",
       "N=10 Accuracy          90.35% ± 8.67   91.55% ± 8.21  64.15% ± 12.71   \n",
       "N=10 Top-5 Accuracy    97.55% ± 4.50   97.75% ± 4.49   92.75% ± 6.72   \n",
       "N=20 Accuracy          86.67% ± 7.81   87.68% ± 7.95   56.17% ± 9.49   \n",
       "N=20 Top-5 Accuracy    96.10% ± 3.99   96.05% ± 3.97   85.32% ± 6.56   \n",
       "N=50 Accuracy          82.22% ± 5.16   82.07% ± 4.16   43.74% ± 5.90   \n",
       "N=50 Top-5 Accuracy    93.16% ± 3.60   92.99% ± 3.19   70.78% ± 5.69   \n",
       "N=100 Accuracy         77.37% ± 3.94   77.73% ± 3.86   35.94% ± 3.69   \n",
       "N=100 Top-5 Accuracy   90.39% ± 2.99   90.00% ± 2.89   60.44% ± 4.05   \n",
       "N=250 Accuracy         70.51% ± 2.74   71.11% ± 2.78             NaN   \n",
       "N=250 Top-5 Accuracy   85.65% ± 2.19   85.50% ± 2.20             NaN   \n",
       "N=10 Accuracy         58.25% ± 13.16  52.60% ± 13.59  83.50% ± 11.15   \n",
       "N=10 Top-5 Accuracy    88.85% ± 9.38   87.35% ± 8.84   96.75% ± 4.82   \n",
       "N=20 Accuracy         45.98% ± 11.06   41.62% ± 9.77   77.92% ± 8.70   \n",
       "N=20 Top-5 Accuracy    76.40% ± 8.26   73.10% ± 9.04   93.40% ± 4.70   \n",
       "N=50 Accuracy          34.93% ± 6.33   30.62% ± 5.84   70.10% ± 6.62   \n",
       "N=50 Top-5 Accuracy    62.19% ± 5.96   57.20% ± 6.55   89.28% ± 3.78   \n",
       "N=100 Accuracy         28.06% ± 4.26   24.88% ± 3.35   61.38% ± 4.34   \n",
       "N=100 Top-5 Accuracy   50.18% ± 4.94   47.44% ± 4.13   83.51% ± 2.92   \n",
       "N=250 Accuracy         20.90% ± 2.09   17.05% ± 1.96             NaN   \n",
       "N=250 Top-5 Accuracy   38.08% ± 2.73   33.81% ± 2.44             NaN   \n",
       "N=10 Accuracy         25.40% ± 11.48  21.30% ± 10.38  26.90% ± 12.22   \n",
       "N=10 Top-5 Accuracy   70.40% ± 14.10  68.10% ± 14.40  70.55% ± 10.84   \n",
       "N=20 Accuracy          18.40% ± 8.76   13.77% ± 6.26   16.93% ± 7.38   \n",
       "N=20 Top-5 Accuracy   47.82% ± 11.84   42.43% ± 9.20   48.10% ± 9.37   \n",
       "N=50 Accuracy          10.47% ± 4.12    8.56% ± 3.42    8.94% ± 3.49   \n",
       "N=50 Top-5 Accuracy    28.72% ± 6.13   24.13% ± 5.72   27.94% ± 4.98   \n",
       "N=100 Accuracy          6.84% ± 1.87    5.47% ± 2.13    6.29% ± 2.05   \n",
       "N=100 Top-5 Accuracy   19.39% ± 3.83   15.86% ± 3.45   18.55% ± 3.64   \n",
       "N=250 Accuracy          4.27% ± 1.06    3.05% ± 0.89             NaN   \n",
       "N=250 Top-5 Accuracy   11.87% ± 1.69    8.95% ± 1.53             NaN   \n",
       "N=10 Accuracy         44.65% ± 15.02  39.20% ± 13.61  44.05% ± 13.98   \n",
       "N=10 Top-5 Accuracy   76.55% ± 11.24   72.50% ± 9.39  77.90% ± 10.82   \n",
       "N=20 Accuracy         36.95% ± 10.37   34.42% ± 8.93   37.15% ± 9.89   \n",
       "N=20 Top-5 Accuracy    61.33% ± 9.41   58.30% ± 8.56   63.62% ± 8.03   \n",
       "N=50 Accuracy          27.65% ± 5.61   24.53% ± 4.80   28.91% ± 6.14   \n",
       "N=50 Top-5 Accuracy    45.89% ± 6.49   41.34% ± 6.19   50.39% ± 6.47   \n",
       "N=100 Accuracy         23.30% ± 4.01   21.30% ± 3.82   24.23% ± 3.31   \n",
       "N=100 Top-5 Accuracy   38.09% ± 4.46   34.96% ± 4.17   42.11% ± 3.73   \n",
       "N=250 Accuracy         18.77% ± 1.99   16.76% ± 1.86             NaN   \n",
       "N=250 Top-5 Accuracy   30.03% ± 2.29   26.94% ± 2.49             NaN   \n",
       "\n",
       "                          data_mails          Model  \n",
       "N=10 Accuracy          28.65% ± 9.79      Model_all  \n",
       "N=10 Top-5 Accuracy   68.85% ± 10.02      Model_all  \n",
       "N=20 Accuracy                    NaN      Model_all  \n",
       "N=20 Top-5 Accuracy              NaN      Model_all  \n",
       "N=50 Accuracy                    NaN      Model_all  \n",
       "N=50 Top-5 Accuracy              NaN      Model_all  \n",
       "N=100 Accuracy                   NaN      Model_all  \n",
       "N=100 Top-5 Accuracy             NaN      Model_all  \n",
       "N=250 Accuracy                   NaN      Model_all  \n",
       "N=250 Top-5 Accuracy             NaN      Model_all  \n",
       "N=10 Accuracy         26.25% ± 11.76    Model_blogs  \n",
       "N=10 Top-5 Accuracy   67.65% ± 13.52    Model_blogs  \n",
       "N=20 Accuracy                    NaN    Model_blogs  \n",
       "N=20 Top-5 Accuracy              NaN    Model_blogs  \n",
       "N=50 Accuracy                    NaN    Model_blogs  \n",
       "N=50 Top-5 Accuracy              NaN    Model_blogs  \n",
       "N=100 Accuracy                   NaN    Model_blogs  \n",
       "N=100 Top-5 Accuracy             NaN    Model_blogs  \n",
       "N=250 Accuracy                   NaN    Model_blogs  \n",
       "N=250 Top-5 Accuracy             NaN    Model_blogs  \n",
       "N=10 Accuracy         21.80% ± 10.62    Model_books  \n",
       "N=10 Top-5 Accuracy   63.40% ± 11.51    Model_books  \n",
       "N=20 Accuracy                    NaN    Model_books  \n",
       "N=20 Top-5 Accuracy              NaN    Model_books  \n",
       "N=50 Accuracy                    NaN    Model_books  \n",
       "N=50 Top-5 Accuracy              NaN    Model_books  \n",
       "N=100 Accuracy                   NaN    Model_books  \n",
       "N=100 Top-5 Accuracy             NaN    Model_books  \n",
       "N=250 Accuracy                   NaN    Model_books  \n",
       "N=250 Top-5 Accuracy             NaN    Model_books  \n",
       "N=10 Accuracy         24.90% ± 12.43    Model_mails  \n",
       "N=10 Top-5 Accuracy   68.80% ± 12.65    Model_mails  \n",
       "N=20 Accuracy                    NaN    Model_mails  \n",
       "N=20 Top-5 Accuracy              NaN    Model_mails  \n",
       "N=50 Accuracy                    NaN    Model_mails  \n",
       "N=50 Top-5 Accuracy              NaN    Model_mails  \n",
       "N=100 Accuracy                   NaN    Model_mails  \n",
       "N=100 Top-5 Accuracy             NaN    Model_mails  \n",
       "N=250 Accuracy                   NaN    Model_mails  \n",
       "N=250 Top-5 Accuracy             NaN    Model_mails  \n",
       "N=10 Accuracy         23.85% ± 10.46  Model_RoBERTa  \n",
       "N=10 Top-5 Accuracy   63.00% ± 11.45  Model_RoBERTa  \n",
       "N=20 Accuracy                    NaN  Model_RoBERTa  \n",
       "N=20 Top-5 Accuracy              NaN  Model_RoBERTa  \n",
       "N=50 Accuracy                    NaN  Model_RoBERTa  \n",
       "N=50 Top-5 Accuracy              NaN  Model_RoBERTa  \n",
       "N=100 Accuracy                   NaN  Model_RoBERTa  \n",
       "N=100 Top-5 Accuracy             NaN  Model_RoBERTa  \n",
       "N=250 Accuracy                   NaN  Model_RoBERTa  \n",
       "N=250 Top-5 Accuracy             NaN  Model_RoBERTa  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.DataFrame()\n",
    "for model, d in data_dict.items():\n",
    "    sub_frame = pd.DataFrame(d)\n",
    "    sub_frame['Model'] = model\n",
    "\n",
    "    dataframe = pd.concat([dataframe, sub_frame], axis=0)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "              &                      &        data\\_all &      data\\_blogs &      data\\_books &      data\\_mails \\\\\n",
      "Model & index &                 &                 &                 &                 \\\\\n",
      "\\midrule\n",
      "Model\\_all & N=10 Accuracy &   91.25\\% ± 7.69 &   90.60\\% ± 8.90 &  87.75\\% ± 10.08 &   28.65\\% ± 9.79 \\\\\n",
      "              & N=10 Top-5 Accuracy &   98.35\\% ± 3.47 &   98.25\\% ± 4.02 &   98.85\\% ± 3.31 &  68.85\\% ± 10.02 \\\\\n",
      "              & N=20 Accuracy &   86.60\\% ± 7.16 &   88.08\\% ± 6.68 &   81.45\\% ± 7.21 &             NaN \\\\\n",
      "              & N=20 Top-5 Accuracy &   95.40\\% ± 4.28 &   96.70\\% ± 3.59 &   96.28\\% ± 3.86 &             NaN \\\\\n",
      "              & N=50 Accuracy &   83.72\\% ± 4.60 &   82.51\\% ± 4.78 &   72.14\\% ± 5.34 &             NaN \\\\\n",
      "              & N=50 Top-5 Accuracy &   93.73\\% ± 3.54 &   93.13\\% ± 3.58 &   91.94\\% ± 3.26 &             NaN \\\\\n",
      "              & N=100 Accuracy &   78.39\\% ± 3.99 &   77.95\\% ± 3.95 &   65.07\\% ± 3.89 &             NaN \\\\\n",
      "              & N=100 Top-5 Accuracy &   90.84\\% ± 2.55 &   90.14\\% ± 2.90 &   86.49\\% ± 2.81 &             NaN \\\\\n",
      "              & N=250 Accuracy &   72.39\\% ± 2.39 &   71.56\\% ± 2.45 &             NaN &             NaN \\\\\n",
      "              & N=250 Top-5 Accuracy &   86.73\\% ± 2.07 &   85.77\\% ± 1.92 &             NaN &             NaN \\\\\n",
      "Model\\_blogs & N=10 Accuracy &   90.35\\% ± 8.67 &   91.55\\% ± 8.21 &  64.15\\% ± 12.71 &  26.25\\% ± 11.76 \\\\\n",
      "              & N=10 Top-5 Accuracy &   97.55\\% ± 4.50 &   97.75\\% ± 4.49 &   92.75\\% ± 6.72 &  67.65\\% ± 13.52 \\\\\n",
      "              & N=20 Accuracy &   86.67\\% ± 7.81 &   87.68\\% ± 7.95 &   56.17\\% ± 9.49 &             NaN \\\\\n",
      "              & N=20 Top-5 Accuracy &   96.10\\% ± 3.99 &   96.05\\% ± 3.97 &   85.32\\% ± 6.56 &             NaN \\\\\n",
      "              & N=50 Accuracy &   82.22\\% ± 5.16 &   82.07\\% ± 4.16 &   43.74\\% ± 5.90 &             NaN \\\\\n",
      "              & N=50 Top-5 Accuracy &   93.16\\% ± 3.60 &   92.99\\% ± 3.19 &   70.78\\% ± 5.69 &             NaN \\\\\n",
      "              & N=100 Accuracy &   77.37\\% ± 3.94 &   77.73\\% ± 3.86 &   35.94\\% ± 3.69 &             NaN \\\\\n",
      "              & N=100 Top-5 Accuracy &   90.39\\% ± 2.99 &   90.00\\% ± 2.89 &   60.44\\% ± 4.05 &             NaN \\\\\n",
      "              & N=250 Accuracy &   70.51\\% ± 2.74 &   71.11\\% ± 2.78 &             NaN &             NaN \\\\\n",
      "              & N=250 Top-5 Accuracy &   85.65\\% ± 2.19 &   85.50\\% ± 2.20 &             NaN &             NaN \\\\\n",
      "Model\\_books & N=10 Accuracy &  58.25\\% ± 13.16 &  52.60\\% ± 13.59 &  83.50\\% ± 11.15 &  21.80\\% ± 10.62 \\\\\n",
      "              & N=10 Top-5 Accuracy &   88.85\\% ± 9.38 &   87.35\\% ± 8.84 &   96.75\\% ± 4.82 &  63.40\\% ± 11.51 \\\\\n",
      "              & N=20 Accuracy &  45.98\\% ± 11.06 &   41.62\\% ± 9.77 &   77.92\\% ± 8.70 &             NaN \\\\\n",
      "              & N=20 Top-5 Accuracy &   76.40\\% ± 8.26 &   73.10\\% ± 9.04 &   93.40\\% ± 4.70 &             NaN \\\\\n",
      "              & N=50 Accuracy &   34.93\\% ± 6.33 &   30.62\\% ± 5.84 &   70.10\\% ± 6.62 &             NaN \\\\\n",
      "              & N=50 Top-5 Accuracy &   62.19\\% ± 5.96 &   57.20\\% ± 6.55 &   89.28\\% ± 3.78 &             NaN \\\\\n",
      "              & N=100 Accuracy &   28.06\\% ± 4.26 &   24.88\\% ± 3.35 &   61.38\\% ± 4.34 &             NaN \\\\\n",
      "              & N=100 Top-5 Accuracy &   50.18\\% ± 4.94 &   47.44\\% ± 4.13 &   83.51\\% ± 2.92 &             NaN \\\\\n",
      "              & N=250 Accuracy &   20.90\\% ± 2.09 &   17.05\\% ± 1.96 &             NaN &             NaN \\\\\n",
      "              & N=250 Top-5 Accuracy &   38.08\\% ± 2.73 &   33.81\\% ± 2.44 &             NaN &             NaN \\\\\n",
      "Model\\_mails & N=10 Accuracy &  25.40\\% ± 11.48 &  21.30\\% ± 10.38 &  26.90\\% ± 12.22 &  24.90\\% ± 12.43 \\\\\n",
      "              & N=10 Top-5 Accuracy &  70.40\\% ± 14.10 &  68.10\\% ± 14.40 &  70.55\\% ± 10.84 &  68.80\\% ± 12.65 \\\\\n",
      "              & N=20 Accuracy &   18.40\\% ± 8.76 &   13.77\\% ± 6.26 &   16.93\\% ± 7.38 &             NaN \\\\\n",
      "              & N=20 Top-5 Accuracy &  47.82\\% ± 11.84 &   42.43\\% ± 9.20 &   48.10\\% ± 9.37 &             NaN \\\\\n",
      "              & N=50 Accuracy &   10.47\\% ± 4.12 &    8.56\\% ± 3.42 &    8.94\\% ± 3.49 &             NaN \\\\\n",
      "              & N=50 Top-5 Accuracy &   28.72\\% ± 6.13 &   24.13\\% ± 5.72 &   27.94\\% ± 4.98 &             NaN \\\\\n",
      "              & N=100 Accuracy &    6.84\\% ± 1.87 &    5.47\\% ± 2.13 &    6.29\\% ± 2.05 &             NaN \\\\\n",
      "              & N=100 Top-5 Accuracy &   19.39\\% ± 3.83 &   15.86\\% ± 3.45 &   18.55\\% ± 3.64 &             NaN \\\\\n",
      "              & N=250 Accuracy &    4.27\\% ± 1.06 &    3.05\\% ± 0.89 &             NaN &             NaN \\\\\n",
      "              & N=250 Top-5 Accuracy &   11.87\\% ± 1.69 &    8.95\\% ± 1.53 &             NaN &             NaN \\\\\n",
      "Model\\_RoBERTa & N=10 Accuracy &  44.65\\% ± 15.02 &  39.20\\% ± 13.61 &  44.05\\% ± 13.98 &  23.85\\% ± 10.46 \\\\\n",
      "              & N=10 Top-5 Accuracy &  76.55\\% ± 11.24 &   72.50\\% ± 9.39 &  77.90\\% ± 10.82 &  63.00\\% ± 11.45 \\\\\n",
      "              & N=20 Accuracy &  36.95\\% ± 10.37 &   34.42\\% ± 8.93 &   37.15\\% ± 9.89 &             NaN \\\\\n",
      "              & N=20 Top-5 Accuracy &   61.33\\% ± 9.41 &   58.30\\% ± 8.56 &   63.62\\% ± 8.03 &             NaN \\\\\n",
      "              & N=50 Accuracy &   27.65\\% ± 5.61 &   24.53\\% ± 4.80 &   28.91\\% ± 6.14 &             NaN \\\\\n",
      "              & N=50 Top-5 Accuracy &   45.89\\% ± 6.49 &   41.34\\% ± 6.19 &   50.39\\% ± 6.47 &             NaN \\\\\n",
      "              & N=100 Accuracy &   23.30\\% ± 4.01 &   21.30\\% ± 3.82 &   24.23\\% ± 3.31 &             NaN \\\\\n",
      "              & N=100 Top-5 Accuracy &   38.09\\% ± 4.46 &   34.96\\% ± 4.17 &   42.11\\% ± 3.73 &             NaN \\\\\n",
      "              & N=250 Accuracy &   18.77\\% ± 1.99 &   16.76\\% ± 1.86 &             NaN &             NaN \\\\\n",
      "              & N=250 Top-5 Accuracy &   30.03\\% ± 2.29 &   26.94\\% ± 2.49 &             NaN &             NaN \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1586346/2621851513.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print(dataframe.reset_index().set_index(['Model','index']).to_latex())\n"
     ]
    }
   ],
   "source": [
    "print(dataframe.reset_index().set_index(['Model','index']).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] Evaluating model - RoBERTa\n",
      "[log] Evaluating on data - all\n",
      "[log] Running 100 repetitions for N=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0ed23e53a084f9f8fc2f6bde4afc715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:all_n:10: 44.65% 76.55%\n",
      "[log] Running 100 repetitions for N=20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351fae5593224adba12d551e70b0452c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:all_n:20: 36.95% 61.33%\n",
      "[log] Running 100 repetitions for N=50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3efdd2eabc46a6b6ee106847a3ec8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:all_n:50: 27.65% 45.89%\n",
      "[log] Running 100 repetitions for N=100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b43327d3f874ba488564077105cbad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:all_n:100: 23.30% 38.09%\n",
      "[log] Running 100 repetitions for N=250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88163de980c142c8b780107554212f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:all_n:250: 18.77% 30.03%\n",
      "[log] Evaluating on data - blogs\n",
      "[log] Running 100 repetitions for N=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc0e036b634b3ba02ed37202922a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:blogs_n:10: 39.20% 72.50%\n",
      "[log] Running 100 repetitions for N=20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b8089e31d4483e8c07ceb1a7f82680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:blogs_n:20: 34.42% 58.30%\n",
      "[log] Running 100 repetitions for N=50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "858eb336ebcb45c791cdb064ebbfbc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:blogs_n:50: 24.53% 41.34%\n",
      "[log] Running 100 repetitions for N=100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02cf68d8c174f8aa087e347c129d1ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:blogs_n:100: 21.30% 34.96%\n",
      "[log] Running 100 repetitions for N=250\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10217ac1aef94c03a3842e1411747fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:blogs_n:250: 16.76% 26.94%\n",
      "[log] Evaluating on data - books\n",
      "[log] Running 100 repetitions for N=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69a04cf12eb44c091a57e4e25a10e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:books_n:10: 44.05% 77.90%\n",
      "[log] Running 100 repetitions for N=20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dad6bfa93ef48f4a2e2734ab6a99768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:books_n:20: 37.15% 63.62%\n",
      "[log] Running 100 repetitions for N=50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788928e8ff8d4ae7b803316989cbddb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:books_n:50: 28.91% 50.39%\n",
      "[log] Running 100 repetitions for N=100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ca8fac90774371bea98f92f3837aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:books_n:100: 24.23% 42.11%\n",
      "[log] Running 100 repetitions for N=250\n",
      "[log] Evaluating on data - mails\n",
      "[log] Running 100 repetitions for N=10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47665a46e8e64f5491aba5d75698bb36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[log] model:RoBERTa_data:mails_n:10: 23.85% 63.00%\n",
      "[log] Running 100 repetitions for N=20\n",
      "[log] Running 100 repetitions for N=50\n",
      "[log] Running 100 repetitions for N=100\n",
      "[log] Running 100 repetitions for N=250\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "DEVICE = 1\n",
    "REPEATS = 100\n",
    "TOP_K = 5\n",
    "\n",
    "n_list = [10, 20, 50, 100, 250]\n",
    "keys = ['all', 'blogs', 'books', 'mails']\n",
    "\n",
    "model = AutoModel.from_pretrained('roberta-large').cuda(DEVICE)\n",
    "\n",
    "data_dict[f'Model_RoBERTa'] = {}\n",
    "print(f'[log] Evaluating model - RoBERTa')\n",
    "\n",
    "for k_data in keys:\n",
    "    data = data_zoo[k_data]\n",
    "        \n",
    "    data_dict[f'Model_RoBERTa'][f'data_{k_data}'] = {}\n",
    "    print(f'[log] Evaluating on data - {k_data}')\n",
    "\n",
    "    for n in n_list:\n",
    "        print(f'[log] Running {REPEATS} repetitions for N={n}')\n",
    "        max_len = len(data.id.unique())\n",
    "        \n",
    "        if n == 'max':\n",
    "            n = max_len\n",
    "\n",
    "        if n <= max_len:\n",
    "            acc, topk, acc_sd, topk_sd = evaluate(model, data, top_k=TOP_K, N=n, repetitions=REPEATS, embed_f=embed_transformer)\n",
    "            data_dict[f'Model_RoBERTa'][f'data_{k_data}'][f'N={n} Accuracy'] = f'{100*acc:.2f}% ± {100*acc_sd:.2f}'\n",
    "            data_dict[f'Model_RoBERTa'][f'data_{k_data}'][f'N={n} Top-5 Accuracy'] = f'{100*topk:.2f}% ± {100*topk_sd:.2f}'\n",
    "\n",
    "            print(f'[log] model:RoBERTa_data:{k_data}_n:{n}: {100*acc:.2f}% {100*topk:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
